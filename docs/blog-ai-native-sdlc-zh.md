# 一次实验：AI 原生的软件开发生命周期，能否像人类一样"打磨"软件

*再一次测试左移*

---

## 实验初衷

最初的目标并不是要构建一个身份认证平台。我想探究一个更根本的问题：**用 AI 原生的开发流程，真的能够打磨出合格的应用吗？**

不是玩具 demo，也不是周末项目。而是一个真正的系统——足够复杂来检验这套方法论，安全要求足够高，"差不多能用"的标准在这里行不通。

我选择了 IAM（身份认证与访问管理）作为实验对象。这不是简单的增删改查：多租户数据隔离、OIDC/OAuth2 流程、Token Exchange、层级化 RBAC 权限、Webhook 签名验证、审计日志——这些环环相扣的复杂性意味着，一个错误决策会引发十几个隐蔽的 bug。在这里，安全不是锦上添花，而是系统存在的根本意义。

最终成果是 [Auth9](https://github.com/gpgkd906/auth9)，一个自托管的 Auth0 替代方案，基于 Rust、React Router 7、TiDB 和 Keycloak 构建。但 Auth9 只是产出物，实验的真正主角是这套开发流水线本身。

## 真正的挑战：可验证性

AI 编码工具确实让写代码更快了。GitHub Copilot、Cursor、Claude Code——它们都兑现了这个承诺。

但写代码从来不是真正的难点。难点在于**如何知道代码是否正确**——而且要足够快、足够自动化，不能让验证本身成为瓶颈。

你能验证一次重构没有破坏三个看似不相关的流程吗？你能确认一个安全修复没有让权限模型倒退吗？你能在路由变更时，同步更新 156 份测试文档——不是下周，而是*现在*，就在同一个开发周期内完成吗？人类当然可以完成这些验证，但需要时间——而时间正是 AI 原生开发要解决的问题。

AI 原生的开发流程不是要消除验证工作，而是要让验证**足够系统化、足够自动化**，跟上 AI 生成代码的速度。如果 AI 写代码快了 10 倍，但验证还是手动操作，那你只是制造了一个 10 倍大的 QA 积压。

## 测试左移

先说清楚：**测试没有消失，反而变得更加重要了。** 变化的是形式。

传统的自动化测试——单元测试、集成测试、端到端测试——在代码库中依然存在。`cargo test` 跑得很快，而且不依赖外部服务。Playwright 驱动端到端流程。Vitest 覆盖前端。这些技术手段依然有效，依然必要。当然，它们都是 AI 自动生成的。

我们在代码级测试*之前*增加了一层：**QA 测试文档**。这些是结构化的规格说明，描述要测什么、怎么测、如何在数据层验证正确性。AI 生成这些文档，人类审查和批准。然后 AI 执行测试——包括浏览器自动化、API 调用、数据库查询和 gRPC 验证。实际上它更接近于传统的手动测试，但却是自动化的。当然，它暂时还不能完全代替手动测试。

这就是测试左移到文档的实践。测试计划不再是功能上线后才补的事后工作。它是在功能规划后产生的第一个产物，驱动后续所有工作：写什么代码、验证什么、出问题时捕获什么。

人类的角色：审查每份生成的测试文档，检查完整性、边界情况，以及 AI 可能遗漏的安全考量；观察 agent 的自动测试，检查它的测试行为是否符合预期，或者说是否存在作弊。AI 的角色：生成文档、执行测试、报告失败、修复力所能及的问题。而 QA 测试文档，则反映了用户故事本身。

## 闭环流水线

整个流水线将 16 个 Agent Skills 串联起来，每个阶段的输出都是下一个阶段的输入：

```
人类 + AI ──► 规划功能
                  │
                  ▼
          ┌─ 生成 QA / 安全 / UIUX 测试文档
          │   (qa-doc-gen)
          ▼
          ┌─ 自动执行测试
          │   浏览器自动化、API 测试、
          │   数据库验证、gRPC 回归测试、
          │   性能基准测试
          ▼
          ┌─ 失败？创建结构化工单
          │   (docs/ticket/)
          ▼
          ┌─ AI 读取工单 → 验证问题 →
          │   修复代码 → 重置环境 →
          │   重新运行测试 → 关闭工单
          │   (ticket-fix)
          ▼
          ┌─ 定期审计文档质量
          │   (qa-doc-governance)
          ▼
          ┌─ 重构后对齐测试
          │   (align-tests, test-coverage)
          ▼
          ┌─ 部署到 Kubernetes
          │   (deploy-gh-k8s)
          └─────────────────────────
```

每个 skill 都是一个独立的 Agent Skill 定义——一个 Markdown 文件，包含指令、工作流步骤和工具脚本：

| 阶段 | Skills | 职责 |
|------|--------|------|
| **规划** | `project-bootstrap` | 从零开始搭建新项目脚手架 |
| **编码** | `rust-conventions`, `keycloak-theme` | 编码规范、主题开发 |
| **测试文档** | `qa-doc-gen`, `qa-doc-governance` | 生成和管理测试文档 |
| **执行测试** | `qa-testing`, `e2e-testing`, `performance-testing`, `auth9-grpc-regression` | QA、端到端、压力测试、gRPC 测试 |
| **修复** | `ticket-fix`, `align-tests` | 自动修复工单、重构后对齐测试 |
| **覆盖率** | `test-coverage` | 确保所有层 >=90% 覆盖率 |
| **部署** | `deploy-gh-k8s` | GitHub Actions 门禁 → K8s 部署 → 健康检查 |
| **运维** | `ops`, `reset-local-env` | 日志、故障排查、环境重置 |

关键洞察：**Agent Skills 可以组合使用**。功能规划完成后，`qa-doc-gen` 生成测试文档；`qa-testing` 执行测试；失败变成 `docs/ticket/` 里的工单；`ticket-fix` 读取工单、修复代码、重置环境、重新运行测试、关闭工单；`qa-doc-governance` 定期审计所有内容，防止文档腐化。

## 文档就是可执行的规格说明

这套方法能够跑通，核心原因是测试文档的结构化——既给人看，也能被 AI 解析。以下是租户 CRUD 测试套件中的一个真实场景：

> ### 场景 1：租户管理入口可见性与创建租户
> 
> **初始状态**
> - 用户已登录管理后台
> - 数据库中无同名或同 slug 的租户
> 
> **测试操作流程**
> 1. 在管理后台左侧导航确认存在「租户管理」菜单入口
> 2. 点击「租户管理」菜单进入租户列表
> 3. 点击「创建租户」按钮
> 4. 填写表单：
>    - 租户名称：`测试公司`
>    - Slug：`test-company`
>    - Logo URL：`https://example.com/logo.png`
> 5. 点击「创建」按钮
> 
> **预期结果**
> - 显示创建成功提示
> - 侧边栏入口可见且可点击进入
> - 租户出现在列表中，状态为「Active」
> 
> **预期数据状态**
> ```sql
> SELECT id, name, slug, logo_url, status FROM tenants WHERE slug = 'test-company';
> -- 预期: 存在一条记录，status = 'active'
> 
> SELECT action, resource_type FROM audit_logs WHERE resource_type = 'tenant' ORDER BY created_at DESC LIMIT 1;
> -- 预期: action = 'tenant.create'
> ```

SQL 验证是关键。AI 不只是检查 UI 是否显示"成功"——它会直接查询数据库，确认数据确实正确落库。这样能抓到一整类 bug：前端显示成功，但后端静默失败。

我们有 **156 份**这样的文档：96 份 QA、48 份安全、12 份 UI/UX——每一份都是可执行的规格说明，由 AI 生成、人类审查。

我们选择文件系统作为信息交换的媒介（例如 `docs/qa`、`docs/ticket` 目录），原因也很简单——因为 Agent 很擅长于 bash 操作，对于文件系统的读写既自然又精准高效。这种基于文件的工作流让整个验证过程变得透明且可追溯。

## 自愈循环：AI 如何"打磨"软件

`ticket-fix` skill 是流水线中最有意思的部分，也是 AI "打磨"软件的核心机制。测试失败后，系统创建结构化工单，然后：

1. **读取工单并验证** — 解析场景、步骤、预期/实际结果、环境信息、SQL 检查。
2. **复现** — 对当前实现运行失败的测试。
3. **修复（如需要）** — 实现最小范围的代码修复。
4. **重置环境** — 必须的。运行 `./scripts/reset-docker.sh` 确保干净状态。
5. **重新运行 QA 步骤** — 按照工单的精确步骤和 SQL 验证执行，采集证据。
6. **误报分析** — 这是让整套系统真正跑起来的关键细节。
7. **关闭工单** — 删除工单文件，总结结果。

### 误报分析

不是每个失败的测试都是真正的 bug。`ticket-fix` skill 显式处理误报——由测试流程缺陷而非代码缺陷导致的失败：

- 测试命令缺少必要的认证头（如 HMAC 签名）
- 前置条件不完整或有歧义（"webhook 必须存在"但没给创建步骤）
- 环境假设与 Docker 默认配置不匹配
- 测试数据引用不存在的实体，没有兜底处理

确认误报后，skill 不只是关闭工单——它会**更新 QA 文档**，防止同样的误报再次出现。把隐式要求改为显式、确保示例命令开箱即用、添加常见失败模式的排障表。

这意味着**每次测试失败都会让测试套件变得更好**，不管失败是真正的 bug 还是测试本身的问题。这正是"打磨"的本质——通过反复的验证和修正，让软件和测试文档都变得更加完善。

## 文档治理

测试文档会腐化。路由变了，API 演进了，权限体系重构了——突然间一半的测试套件在测试不存在的行为。

由于所有 QA 测试文档会由 agent 反复执行，如果文档过时，我们会注意到大量的伪阳性工单报告。让文档保持更新的最好策略就是不断使用它。

`qa-doc-governance` skill 进行定期审计，按严重等级分类：

- **P0**：导航不通或测试流程不可用（缺少 checklist、误导性的认证步骤、索引偏移）
- **P1**：治理偏移（单文件超过 5 个场景、UI 相关文档缺少入口可见性说明）
- **P2**：风格一致性（命名对齐、措辞规范化）

工作流：全量 lint → 分级分类 → 按优先级修复 → 同步索引和 manifest → 验证并生成报告。可以在任意新功能或重构后运行，也可以定期执行（如每周一次），大版本发布前运行一次，事故后运行一次。

一条硬性规则：**每份文档必须包含回归测试 checklist**。没有例外。即使文档其他部分漂移了，checklist 仍提供最低可用的验证路径。

## 跨文档影响分析

一个功能变更不只影响一份测试文档。`qa-doc-gen` skill 包含强制性的跨文档影响分析，扫描全部 156 份文档，查找：

- 路由变更（`/dashboard`、`/tenant/select`、`/auth/callback`）
- Token 模型变更（`id_token`、`tenant token`、`token exchange`）
- 权限/认证变更（`401`、`403`、`scope`、`audience`、`role`）
- UI 导航文案和预期重定向路径

对每份受影响的文档，分类处理：

1. **需要修补** — 更新步骤、预期结果或安全断言
2. **需要备注** — 添加前置条件说明或分支路径说明，避免测试者困惑
3. **无需变更** — 显式记录为什么不受影响

核心规则：**永远不要只创建新文档而忽略已过时的旧文档。**

## 人类到底在做什么

这是人机协作，不是替代。以下是我日常实际做的事情：

- **规划**：决定做什么、定义验收标准、选择架构权衡。AI 搭脚手架，我掌方向。
- **审查**：我审查生成的测试文档，以及新功能/重构的第一版代码。QA 执行和工单修复循环？交给 AI 自主完成。审查精力集中在最关键的地方——定义正确性的*规格*，和确定方向的*初始实现*。
- **纠偏**：流水线产生误报时，我审查根因分析。Governance 标记 P0 时，我决定修复方案。
- **架构**：领域建模、数据流设计、安全边界——这些需要对*系统*的判断力，不只是对代码。

AI 处理了以前占我 60-70% 时间的机械性工作。我专注于需要人类判断力的 30-40%。

这不是说我们可以完全放手让 AI 做所有的事情。经过了 20 轮迭代，我们依然可以看到 AI 的测试会产生工单——但比起最初的版本，这些工单越来越少，应用的细节也越来越丰富。这种体验，和人类工程师做的事情很相似：我们通过反复的 QA 不断消除 bug，对软件进行打磨。区别在于，这个打磨循环现在跑得更快了，而且每一轮都有据可查。

**人类的核心价值在于定义"我们想做什么，不想做什么"**，并提供足够好的品味和判断。而 AI 则负责高效地实现这个定义，并通过自动化测试不断验证和修正。

这里有一个重要的观察：实际上人类专家的角色变得更加关键了——我们需要构建一个能让 Agent 进行验证的环境，这就要求人类专家不仅要懂开发，还得熟悉基础设施。同时，对 DevOps 和安全问题的关注，能帮助人类专家更全面地审查文档，并持续优化整个流程。说白了，我们需要的是真正的全栈工程师。

## 诚实的局限性

这套方法不是魔法。以下是目前做得不够好的地方：

- **全新的架构决策** — AI 擅长实现它见过的模式。对于真正新颖的设计选择（比如我们的 Token Exchange 流程），人类架构师仍然主导。
- **安全审查** — AI 生成安全测试文档并执行，但威胁模型本身需要人类安全专家。自动化测试能捕捉已知模式，但发现不了新型攻击向量。
- **治理的边际递减** — 维护 156 份测试文档的开销终将接近不维护的开销。目前还没到那个点，但曲线不是线性的。
- **流程需要项目适配** — 这套流程本身需要对具体项目进行优化。比如有些项目可能不需要 UI 测试，有些项目可能更侧重 API 或数据层验证，需要根据项目特点调整技能组合和测试重点。

## 数字说话

- **16** 个 Agent Skills，覆盖完整开发生命周期
- **156** 份测试文档（96 QA + 48 安全 + 12 UI/UX）
- **9** 个工具脚本，用于 token 生成、API 测试、gRPC 冒烟测试
- **~2,300** 行 skill 定义
- **1** 个人类

## 为什么选择 IAM？

常见问题：为什么用身份认证平台来测试开发方法论？

因为简单问题检验不了方法论。一个 TODO 应用或博客引擎什么都证明不了——领域太简单，任何方法看起来都不错。IAM 逼你面对：

- **不能糊弄的安全** — Token 验证、权限执行、注入防护。48 份安全测试文档的存在是因为"我本地跑通了"在认证领域不可接受。
- **多系统协调** — Keycloak、TiDB、Redis、Rust 后端、React 前端。Token Exchange 流程里的一个 bug 会触达每一层。
- **状态复杂度** — 多租户数据隔离、层级化 RBAC、会话管理。状态空间大到手动测试无法覆盖。

如果 AI 原生的开发流程能打磨出一个合格的 IAM 平台，那它对大多数应用都适用。

## 自己试试看

 整条流水线开源：[github.com/gpgkd906/auth9](https://github.com/gpgkd906/auth9)

`.agents/skills/` 目录包含全部 16 个 Agent Skills。`docs/qa/`、`docs/security/`、`docs/uiux/` 目录包含测试文档。`project-bootstrap` skill 可以搭建一个同结构的新项目脚手架——Rust 后端、React 前端、Docker Compose、Kubernetes manifest、部署脚本——让你在自己的项目中复制这套方法。

这套方法论不是身份认证平台专用的。它是一种思考 AI 主导开发的方式：**不要只用 AI 更快地写代码——用它闭合"规划、测试、修复、部署"的循环。**

---

*Auth9 是自托管的身份认证和访问管理平台（Auth0 替代方案），基于 Rust、React Router 7、TiDB 和 Keycloak 构建。AI 原生开发流水线使用 Agent Skills 架构配合 16 个自定义技能。*
